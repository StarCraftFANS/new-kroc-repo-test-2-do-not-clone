-- test program for lexer in useful library.

#INCLUDE "useful.module"
#INCLUDE "file.module"

PROC main (CHAN BYTE scr!)
  SEQ
    --{{{  byte-stream reader first
    SEQ
      out.string ("-------[byte-stream read]--------------------------------------------------*n", 0, scr!)
      CHAN P.LEX.BYTESTREAM lb:
      PAR
        stream.file.reader ("test-lexer.txt", lb!)
        stream.bytes.dump (lb?, scr!)
    --}}}
    --{{{  then cut up into lines
    SEQ
      out.string ("-------[line-stream read]--------------------------------------------------*n", 0, scr!)
      CHAN P.LEX.BYTESTREAM lb:
      CHAN P.LEX.LINESTREAM ll:
      PAR
        stream.file.reader ("test-lexer.txt", lb!)
        stream.bytes.to.lines (lb?, ll!)
        stream.lines.dump (ll?, scr!)
    --}}}
    --{{{  then cut up into lines, trimmed and continued
    SEQ
      out.string ("-------[line-stream read with trim+continue]-------------------------------*n", 0, scr!)
      CHAN P.LEX.BYTESTREAM lb:
      CHAN P.LEX.LINESTREAM ll1, ll2, ll3:
      PAR
        stream.file.reader ("test-lexer.txt", lb!)
        stream.bytes.to.lines (lb?, ll1!)
        stream.trim.lines (ll1?, ll2!)
        stream.continue.lines ('\', ll2?, ll3!)
        stream.lines.dump (ll3?, scr!)
    --}}}
    --{{{  then cut up into lines, continued, decommented and trimmed
    SEQ
      out.string ("-------[line-stream read with cont+decomment+trim]-------------------------*n", 0, scr!)
      CHAN P.LEX.BYTESTREAM lb:
      CHAN P.LEX.LINESTREAM ll1, ll2, ll3, ll4:
      PAR
        stream.file.reader ("test-lexer.txt", lb!)
        stream.bytes.to.lines (lb?, ll1!)
        stream.continue.lines ('\', ll1?, ll2!)
        stream.remove.eol.comments ('#', ll2?, ll3!)
        stream.trim.lines (ll3?, ll4!)
        stream.lines.dump (ll4?, scr!)
    --}}}
    --{{{  then cut up into lines, continued, decommented, trimmed and tokenised
    SEQ
      out.string ("-------[line-stream read with cont+decomment+trim+tokens]------------------*n", 0, scr!)
      CHAN P.LEX.BYTESTREAM lb:
      CHAN P.LEX.LINESTREAM ll1, ll2, ll3, ll4:
      CHAN P.LEX.TOKENSTREAM tk1:
      PAR
        stream.file.reader ("test-lexer.txt", lb!)
        stream.bytes.to.lines (lb?, ll1!)
        stream.continue.lines ('\', ll1?, ll2!)
        stream.remove.eol.comments ('#', ll2?, ll3!)
        stream.trim.lines (ll3?, ll4!)
        stream.lines.to.tokens (TRUE, ll4?, tk1!)
        stream.tokens.dump (tk1?, scr!)
    --}}}
    --{{{  tokeniser plain
    SEQ
      out.string ("-------[tokeniser network plain]-------------------------------------------*n", 0, scr!)
      CHAN P.LEX.TOKENSTREAM tk1:
      PAR
        stream.file.tokeniser ("test-lexer.txt", tk1!, FALSE, #00, #00)
        stream.tokens.dump (tk1?, scr!)
    --}}}
    --{{{  tokeniser full
    SEQ
      out.string ("-------[tokeniser network full]--------------------------------------------*n", 0, scr!)
      CHAN P.LEX.TOKENSTREAM tk1:
      PAR
        stream.file.tokeniser ("test-lexer.txt", tk1!, TRUE, '\', '#')
        stream.tokens.dump (tk1?, scr!)
    --}}}
:

